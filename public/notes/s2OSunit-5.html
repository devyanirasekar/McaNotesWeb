<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="notes.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
    <title>SEM2 OS unit-5</title>
</head>
<body>
    
    <nav class="nav-container">
        <div class="text_logo">
            <h2 class="logo">MCA_<span class="notes_text"> Notes</span></h2>
        </div>

        <ul class="list">
            <a href="/index.html"><li class="listno list-diplay">Home</li></a>
            <li class="listno list-diplay">About</li>
            <li class="listno list-diplay">Contact</li>
            <li><span class="download list-diplay" id="download">Download</span></li>
            <li class="toggle"><i class="fa-solid fa-bars fa-lg toggle-btn"></i></li>
        </ul>

        
    </nav>
    <div class="dropDown">
        <ul class="drop-list">
            <a href="/index.html"><li class="listno ">Home</li></a>
            <li class="listno ">About</li>
            <li class="listno">Contact</li>
            <li><span class="download " id="download">Download</span></li></ul>
    </div>

    <!-- .................................................................... -->
    
    <div class="noteContainer">
    
    <!------------------------ paste code here   ---------------------------------- -->
    <div style="margin: 5px 0;">
        <p><b>UNIT-5</b></p>
        <p><strong>&nbsp;</strong></p>
        <p><strong><h2>INTRODUCTION</h2></strong></p>
        <p>Memory is central to the operation of a modern computer system. Memory is a large array of words or bytes, each with its own address.</p>
        <p>A program resides on a disk as a binary executable file. The program must be brought into memory and placed within a process for it to be executed Depending on the memory management in use the process may be moved between disk and memory during its execution. The collection of processes on the disk that are waiting to be brought into memory for execution forms the input queue. i.e. selected one of the process in the input queue and to load that process into memory. We can provide protection by using two registers, usually a base and a limit, as shown in fig. 7.1. the base register holds the smallest legal physical memory address; the limit register specifies the size of the range. For example, if the base register holds 300040 and the limit register is 120900, then the program can legally access all addresses from 300040 through 420939 (inclusive).</p>
        <p><strong> </strong>A base and limit register define a logical address space.</p>
        <h1>MEMORY PARTITIONING</h1>
        <p>The binding of instructions and data to memory addresses can be done at any step along the way:</p>
        <ul>
          <li><strong>Compile time: </strong>If it is known at compile time where the process will reside in memory, then absolute code can be generated.</li>
          <li><strong>Load time: </strong>If it is not known at compile time where the process will reside in memory, then the compiler must generate re-locatable code.</li>
          <li><strong>Execution time: </strong>If the process can be moved during its execution from one memory segment to another, then binding must be delayed until run time.</li>
        </ul>
        <h2>Dynamic Loading</h2>
        Better memory-space utilization can be done by dynamic loading. With dynamic loading, a routine is not loaded until it is called.
      </div>
      <div style="margin: 5px 0;">
        <p>All routines are kept on disk in a re-locatable load format. The main program is loaded into memory and is executed.</p>
        <p>The advantage of dynamic loading is that an unused routine is never loaded.</p>
        <h2>Dynamic Linking</h2>
        <p>Most operating systems support only static linking, in which system language libraries are treated like any other object module and are combined by the loader into the binary program image. The concept of dynamic linking is similar to that of dynamic loading. Rather than loading being postponed until execution time, linking is postponed. This feature is usually used with system libraries, such as language subroutine libraries. With dynamic linking, a stub is included in the image for each library-routine reference. This stub is a small piece of code that indicates how to locate the appropriate memory-resident library routing.</p>
        <p>The entire program and data of a process must be in physical memory for the process to execute. The size of a process is limited to the size of physical memory. So that a process can be larger than the amount of memory allocated to it, a technique called overlays is sometimes used. The idea of overlays is to keep in memory only those instructions and data that are needed at any given time. When other instructions are needed, they are loaded into space that was occupied previously by instructions that are no longer needed.</p>
        <p>Example, consider a two-pass assembler. During pass 1, it constructs a symbol table; then, during pass 2, it generates machine-language code. We may be able to partition such an assembler into pass 1 code, pass 2 code, the symbol table</p>
        <p>1, and common support routines used by both pass 1 and pass 2.</p>
        <p>Let us consider</p>
        <table width="205">
          <tbody>
            <tr>
              <td width="178">
                <p>Pass1</p>
              </td>
              <td width="27">
                <p>70K</p>
              </td>
            </tr>
            <tr>
              <td width="178">
                <p>Pass 2</p>
              </td>
              <td width="27">
                <p>80K</p>
              </td>
            </tr>
            <tr>
              <td width="178">
                <p>Symbol table</p>
              </td>
              <td width="27">
                <p>20K</p>
              </td>
            </tr>
            <tr>
              <td width="178">
                <p>Common routines</p>
              </td>
              <td width="27">
                <p>30K</p>
              </td>
            </tr>
          </tbody>
        </table>
        <p>To load everything at once, we would require 200K of memory. If only 150K is available, we cannot run our process. But pass 1 and pass 2 do not need to be in memory at the same time. We thus define two overlays: Overlay A is the symbol table, common routines, and pass 1, and overlay B is the symbol table, common routines, and pass 2.</p>
        <p>We add an overlay driver (10K) and start with overlay A in memory. When we finish pass 1, we jump to the overlay driver, which reads overlay B into memory, overwriting overlay A, and then transfers control to pass 2. Overlay A needs only 120K, whereas overlay B needs 130K.</p>
        <p>As in dynamic loading, overlays do not require any special support from the operating system.</p>
        <h2> Logical versus Physical Address Space</h2>
        <p>An address generated by the CPU is commonly referred to as a logical address, whereas an address seen by the memory unit is commonly referred to as a physical address.</p>
        <p>The compile-time and load-time address-binding schemes result in an environment where the logical and physical addresses are the same. The execution-time address-binding scheme results in an environment where the logical and physical addresses differ. In this case, we usually refer to the logical address as a virtual address. The set of all logical addresses generated by a program is referred to as a logical address space; the set of all physical addresses corresponding to these logical addresses is referred to as a physical address space.</p>
        <p>The run-time mapping from virtual to physical addresses is done by the memory management unit (MMU), which is a hardware device.</p>
        <p>The base register is called a relocation register. The value in the relocation register is added to every address generated by a user process at the time it is sent to memory. For example, if the base is at 13000, then an attempt by the user to address location 0 dynamically relocated to location 14,000; an access to location 347 is mapped to location 13347. The MS-DOS operating system running on the Intel 80x86 family of processors uses four relocation registers when loading and running processes.</p>
        <p>The user program never sees the real physical addresses. The program can create a pointer to location 347 store it memory, manipulate it, compare it to other addresses all as the number 347.</p>
        <p>The user program deals with logical addresses. The memory-mapping hardware converts logical addresses into physical addressed Logical addresses (in the range 0 to max) and physical addresses (in the range R + 0 to R + max for a base value R). The user generates only logical addresses.</p>
        <p>The concept of a logical address space that is bound to a separate physical address space is central to proper memory management.</p>
        <p>A process, can be swapped temporarily out of memory to a backing store, and then brought back into memory for continued execution. Assume a multiprogramming environment with a round robin CPU-scheduling algorithm. When a quantum expires, the memory manager will start to swap out the process that just finished, and to swap in another process to the memory space that has been freed ( Fig 7.2). When each process finishes its quantum, it will be swapped with another process.</p>
        <p><strong> </strong>Swapping of two processes using a disk as a blocking store</p>
        <p>A variant of this swapping policy is used for priority-based scheduling algorithms. If a higher-priority process arrives and wants service, the memory manager can swap out the lower-priority process so that it can load and execute the higher priority process.</p>
        <p>When the higher priority process finishes, the lower-priority process can be swapped back in and continued. This variant of swapping is</p>
        <p>sometimes called rollout, roll in. A process is swapped out will be swapped back into the same memory space that it occupies previously. If binding is done at assembly or load time, then the process cannot be moved to different location. If execution-time binding is being used, then it is possible to swap a process into a different memory space.</p>
        <p>Swapping requires a backing store. The backing store is commonly a fast disk. It is large enough to accommodate copies of all memory images for all users. The system maintains a ready queue consisting of all processes whose memory images are on the backing store or in memory and are ready to run.</p>
        <p>The context-switch time in such a swapping system is fairly high. Let us assume that the user process is of size 100K and the backing store is a standard hard disk with transfer rate of 1 megabyte per second. The actual transfer of the 100K process to or from memory takes</p>
        <p>100K / 1000K per second = 1/10 second</p>
        <p>= 100 milliseconds</p>
        <h2>&nbsp;&nbsp;&nbsp; CONTIGUOUS ALLOCATION</h2>
        <p>The main memory must accommodate both the operating system and the various user processes. The memory is usually divided into two partitions, one for the resident operating system, and one for the user processes.</p>
        <p>To place the operating system in low memory. Thus, we shall discuss only me situation where the operating system resides in low memory (Figure 8.5). The development of the other situation is similar. Common Operating System is placed in low memory.</p>
        <h2>Single-Partition Allocation</h2>
        <p>If the operating system is residing in low memory, and the user processes are executing in high memory. And operatingsystem code and data are protected from changes by the user processes. We also need protect the user processes from one another. We can provide this 2 protection by using a relocation registers.</p>
        <p>The relocation register contains the value of the smallest physical address; the limit register contains the range of logical addresses (for example, relocation = 100,040 and limit = 74,600). With relocation and limit registers, each logical address must be less than the limit register; the MMU maps the logical address dynamically by adding the value in the relocation register. This mapped address is sent to memory.</p>
        <p>The relocation-register scheme provides an effective way to allow the operating system size to change dynamically.</p>
        <h2> Multiple-Partition Allocation</h2>
        <p>One of the simplest schemes for memory allocation is to divide memory into a number of fixed-sized partitions. Each partition may contain exactly one process. Thus, the degree of multiprogramming is bound by the number of partitions. When a partition is free, a process is selected from the input queue and is loaded into the free partition. When the process terminates, the partition becomes available for another process.</p>
        <p>The operating system keeps a table indicating which parts of memory are available and which are occupied. Initially, all memory is available for user processes, and is considered as one large block, of available memory, a hole. When a process arrives and needs memory, we search for a hole large enough for this process.</p>
        <p>For example, assume that we have 2560K of memory available and a resident operating system of 400K. This situation leaves 2160K for user processes. FCFS job scheduling, we can immediately allocate memory to processes P1, P2, P3. Holes size 260K that cannot be used by any of the remaining processes in the input queue. Using a round-robin CPU-scheduling with a quantum of 1 time unit, process will terminate at time 14, releasing its memory.</p>
        <p>Memory allocation is done using Round-Robin Sequence as shown in fig. When a process arrives and needs memory, we search this set for a hole that is large enough for this process. If the hole is too large, it is split into two: One part is allocated to the arriving process; the other is returned to the set of holes. When a process terminates, it releases its block of memory, which is then placed back in the set of holes. If the new hole is adjacent to other holes, we merge these adjacent holes to form one larger hole.</p>
        <p>This procedure is a particular instance of the general dynamic storage-allocation problem, which is how to satisfy a request of size n from a list of free holes. There are many solutions to this problem. The set of holes is searched to determine which hole is best to allocate, first-fit, best-fit, and worst-fit are the most common strategies used to select a free hole from the set of available holes.</p>
        <ul>
          <li>First-fit: Allocate the first hole that is big enough. Searching can start either at the beginning of the set of holes or where the previous first-fit search ended. We can stop searching as soon as we find a free hole that is large enough.</li>
          <li>Best-fit: Allocate the smallest hole that is big enough. We must search the entire list, unless the list is kept ordered by size. This strategy-produces the smallest leftover hole.</li>
          <li>Worst-fit: Allocate the largest hole. Again, we must search the entire list unless it is sorted by size. This strategy produces the largest leftover hole which may be more useful than the smaller leftover hole from a best-t approach.</li>
        </ul>
        <h2>External and Internal Fragmentation</h2>
        <p>As processes are loaded and removed from memory, the free memory space is broken into little pieces. External fragmentation exists when enough to the memory space exists to satisfy a request, but it is not contiguous; storage is fragmented into a large number of small holes.</p>
        <p>Depending on the total amount of memory storage and the average process size, external fragmentation may be either a minor or a major problem.</p>
        <p>Given N allocated blocks, another 0.5N blocks will be lost due to fragmentation. That is, one-third of memory may be unusable. This property is known as the 50- percent rule.</p>
        <p>Internal fragmentation - memory that is internal to partition, but is not being used.</p>
        <p>External fragmentation is avoided by using paging. In this physical memory is broken into blocks of the same size called pages. When a process is to be executed, its pages are loaded into any available memory frames. Every address generated by the CPU is divided into any two parts: a page number(p) and a page offset(d) (Fig 7.3). The page number is used as an index into a page table. The page table contains the base address of each page in physical memory. This base address is combined with the gage offset to define the physical memory address that is sent to the memory unit.</p>
        <p><strong></strong>Paging Hardware</p>
        <p>The page size like is defined by the hardware. The size of a page is typically a power of 2 varying between 512 bytes and 8192 bytes per page, depending on the computer architecture. The selection of a power of 2 as a page size makes the translation of a logical address into a page number and page offset. lf the size of logical address space is 2<sup>m</sup>, and a page size is 2<sup>n </sup>addressing units (bytes or words), then the high-order m - n bits of a logical address designate the page number, and the n low-order bits designate the page offset. Thus, the logical address is as follows:</p>
        <p>page number</p>
        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; page&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offset</p>
        <p>p&nbsp;&nbsp;&nbsp;&nbsp; d m &ndash; n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; n</p>
        <p>where p is an index into the page table and d is the displacement within the page.</p>
        <p>Paging is a form of dynamic relocation. Every logical address is bound by the paging hardware to some physical address.</p>
        <p>When we use a paging scheme, we have no external fragmentation: Any free frame can be allocated to a process that needs it.</p>
        <p>If process size is independent of page size, we can have internal fragmentation to average one-half page per process.</p>
        <p>When a process arrives in the system to be executed, its size, expressed in pages, is examined. Each page of the process needs one frame. Thus, if the process requires n pages, there must be at least n frames available in memory. If there are n frames available, they are allocated to this arriving process. The first page of the process is loaded into one of the allocated frames and the frame number is put in the page table for this process. The next page is loaded into another frame, and its frame number is put into the page table, and so on.</p>
        <p>The user program views that memory as one single contiguous space, containing only this one program. But the user program is scattered throughout physical memory and logical addresses are translated into physical addresses.</p>
        <p>The operating system is managing physical memory, it must be aware of the allocation details of physical memory: which frames are allocated, which frames are available, how many total frames there are, and so on. This information is generally kept in a data structure called a frame table. The frame table has one entry for each physical page frame, indicating whether the latter is free allocated and, if it is allocated, to which page of which process or processes.</p>
        <p>The operating system maintains a copy of the page table for each process. Paging therefore increases the context-switch time.</p>
        <p>A user program can be subdivided using segmentation, in which the program and its associated data are divided into a number of <strong>segments. </strong>It is not required that all segments of all programs be of the same length, although there is a maximum segment length. As with paging, a logical address using segmentation consists of two parts, in this case a segment number and an offset.</p>
        <p>Because of the use of unequal-size segments, segmentation is similar to dynamic partitioning. In segmentation, a program may occupy more than one partition, and these partitions need not be contiguous. Segmentation eliminates internal fragmentation but, like dynamic partitioning, it suffers from external fragmentation. However, because a process is broken up into a number of smaller pieces, the external fragmentation should be less. Whereas paging is invisible to the programmer, segmentation usually visible and is provided as a convenience for organizing programs and data.</p>
        <p>Another consequence of unequal-size segments is that there is no simple relationship between logical addresses and physical addresses. Segmentation scheme would make use of a segment table for each process and a list of free blocks of main memory. Each segment table entry would have to as in paging give the starting address in main memory of the corresponding segment. The entry should also provide the length of the segment, to assure that invalid addresses are not used. When a process enters the Running state, the address of its segment table is loaded into a special register used by the memory management hardware.</p>
        <p>Consider an address of n + m bits, where the leftmost n bits are the segment number and the rightmost m bits are the offset. The following steps are needed for address translation:</p>
        <ul>
          <li>Extract the segment number as the leftmost n bits of the logical address.</li>
          <li>Use the segment number as an index into the process segment table to find the starting physical address of the segment.</li>
          <li>Compare the offset, expressed in the rightmost m bits, to the length of the segment. If the offset is greater than or equal to the length, the address is invalid.</li>
          <li>The desired physical address is the sum of the starting physical address of the segment plus the offset.</li>
        </ul>
        <p>Segmentation and paging can be combined to have a good result.</p>
        <h1>VIRTUAL MEMORY</h1>
        <ul>
          <li>Virtual memory is a technique that allows the execution of process that may not be completely in memory. The main visible advantage of this scheme is that programs can be larger than physical memory.</li>
          <li>Virtual memory is the separation of user logical memory from physical memory this separation allows an extremely large virtual memory to be provided for programmers when only a smaller physical memory is available (Fig 8.1).</li>
          <li>Following are the situations, when entire program is not required to load fully.</li>
        </ul>
        <ol>
          <li>User written error handling routines are used only when an error occurs in the data or computation.</li>
          <li>Certain options and features of a program may be used rarely.</li>
          <li>Many tables are assigned a fixed amount of address space even though only a small amount of the table is actually used.</li>
        </ol>
        <p> The ability to execute a program that is only partially in memory would counter many benefits.</p>
        <ol>
          <li>Less number of I/O would be needed to load or swap each user program into memory.</li>
          <li>A program would no longer be constrained by the amount of physical memory that is available.</li>
          <li>Each user program could take less physical memory, more programs could be run the same time, with a corresponding increase in CPU utilization and throughput.</li>
        </ol>
        <p><strong> </strong>Diagram showing virtual memory that is larger than physical memory.</p>
        <p>Virtual memory is commonly implemented by demand paging. It can also be implemented in a segmentation system.</p>
        <p>Demand segmentation can also be used to provide virtual memory.</p>
        <p>A demand paging is similar to a paging system with swapping(Fig 8.2). When we want to execute a process, we swap it into memory. Rather than swapping the entire process into memory.</p>
        <p>When a process is to be swapped in, the pager guesses which pages will be used before the process is swapped out again Instead of swapping in a whole process, the pager brings only those necessary pages into memory. Thus, it avoids reading into memory pages that will not be used in anyway, decreasing the swap time and the amount of physical memory needed.</p>
        <p>Hardware support is required to distinguish between those pages that are in memory and those pages that are on the disk using the valid-invalid bit scheme. Where valid and invalid pages can be checked checking the bit and marking a page will have no effect if the process never attempts to access the pages. While the process executes and accesses pages that are memory resident, execution proceeds normally.</p>
        <p><strong></strong>Transfer of a paged memory to continuous disk space</p>
        <p>Access to a page marked invalid causes a page-fault trap. This trap is the result of the operating system's failure to bring the desired page into memory. But page fault can be handled as following (Fig 8.3):</p>
        <p><strong> </strong>Steps in handling a page fault</p>
        <ol>
          <li>We check an internal table for this process to determine whether the reference was a valid or invalid memory access.</li>
          <li>If the reference was invalid, we terminate the process. If .it was valid, but we have not yet brought in that page, we now page in the latter.</li>
          <li>We find a free frame.</li>
          <li>We schedule a disk operation to read the desired page intothe newly allocated frame.</li>
          <li>When the disk read is complete, we modify the internal table kept with the process and the page table to indicate that the page is now in memory.</li>
          <li>We restart the instruction that was interrupted by the illegaladdress trap. The process can now access the page as though it had always been memory.</li>
        </ol>
        <p>Therefore, the operating system reads the desired page into memory and restarts the process as though the page had always been in memory.</p>
        <p>The page replacement is used to make the frame free if they are not in used. If no frame is free then other process is called in.</p>
        <p><strong> Advantages of Demand Paging:</strong></p>
        <ol>
          <li>Large virtual memory.</li>
          <li>More efficient use of memory.</li>
          <li>Unconstrained multiprogramming. There is no limit on degree of multiprogramming.</li>
        </ol>
        <p><strong> Disadvantages of Demand Paging:</strong></p>
        <ol>
          <li>Number of tables and amount of processor over head for handling page interrupts are greater than in the case of the simple paged management techniques.</li>
          <li>due to the lack of an explicit constraints on a jobs address space size.</li>
        </ol>
        <h1> PAGE REPLACEMENT ALGORITHM</h1>
        <p>There are many different page replacement algorithms. We evaluate an algorithm by running it on a particular string of memory reference and computing the number of page faults. The string of memory references is called reference string. Reference strings are generated artificially or by tracing a given system and recording the address of each memory reference. The latter choice produces a large number of data.</p>
        <ol>
          <li>For a&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; given&nbsp;&nbsp; page&nbsp;&nbsp;&nbsp; size&nbsp;&nbsp;&nbsp;&nbsp; we&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; need&nbsp;&nbsp;&nbsp; to&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; consider&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; only&nbsp;&nbsp;&nbsp;&nbsp; the&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; page</li>
        </ol>
        <p>number, not the entire address.</p>
        <ol start="2">
          <li>if we have a reference to a page p, then any immediately following references to page p will never cause a page fault. Page p will be in memory after the first reference; the immediately following references will not fault.</li>
        </ol>
        <p>Eg:- consider the address sequence</p>
        <p>0100, 0432, 0101, 0612, 0102, 0103, 0104, 0101, 0611, 0102, 0103, 0104, 0101, 0610, 0102, 0103, 0104, 0104, 0101, 0609, 0102, 0105 and reduce to 1, 4, 1, 6,1, 6, 1, 6, 1, 6, 1</p>
        <p>To determine the number of page faults for a particular reference string and page replacement algorithm, we also need to know the number of page frames available. As the number of frames available increase, the number of page faults will decrease.</p>
        <h2>FIFO Algorithm</h2>
        <p>The simplest page-replacement algorithm is a FIFO algorithm. A FIFO replacement algorithm associates with each page the time when that page was brought into memory. When a page must be replaced, the oldest page is chosen. We can create a FIFO queue to hold all pages in memory.</p>
        <p>The first three references (7, 0, 1) cause page faults, and are brought into these empty eg. 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2, 1, 2, 0, 1 and consider 3 frames. This replacement means that the next reference to 0 will fault. Page 1 is then replaced by page 0.</p>
        <h2>Optimal Algorithm</h2>
        <p>An optimal page-replacement algorithm has the lowest pagefault rate of all algorithms. An optimal page-replacement algorithm exists, and has been called OPT or MIN. It is simply Replace the page that will not be used for the longest period of time.</p>
        <p>Now consider the same string with 3 empty frames.</p>
        <p>The reference to page 2 replaces page 7, because 7 will not be used until reference 18, whereas page 0 will be used at 5, and page 1 at 14. The reference to page 3 replaces page 1, as page 1 will be the last of the three pages in memory to be referenced again. Optimal replacement is much better than a FIFO.</p>
        <p>The optimal page-replacement algorithm is difficult to implement, because it requires future knowledge of the reference string.</p>
        <h2> LRU Algorithm</h2>
        <p>The FIFO algorithm uses the time when a page was brought into memory; the OPT algorithm uses the time when a page is to be used. In LRU replace the page that has not been used for the longest period of time.</p>
        <p>LRU replacement associates with each page the time of that page's last use. When a page must be replaced, LRU chooses that page that has not been used for the longest period of time.</p>
        <p>Let S<sup>R </sup>be the reverse of a reference string S, then the pagefault rate for the OPT algorithm on S is the same as the page-fault rate for the OPT algorithm on S<sup>R</sup>.</p>
        <h2> LRU Approximation Algorithms</h2>
        <p>Some systems provide no hardware support, and other page-replacement algorithm. Many systems provide some help, however, in the form of a reference bit. The reference bit for a page is set, by the hardware, whenever that page is referenced. Reference bits are associated with each entry in the page table Initially, all bits are cleared (to 0) by the operating system. As a user process executes, the bit associated with each page referenced is set (to 1) by the hardware.</p>
        <h3> Additional-Reference-Bits Algorithm</h3>
        <p>The operating system shifts the reference bit for each page into the high-order or of its 8-bit byte, shifting the other bits right 1 bit, discarding the low-order bit.</p>
        <p>These 8-bit shift registers contain the history of page use for the last eight time periods. If the shift register contains 00000000, then the page has not been used for eight time periods; a page that is used at least once each period would have a shift register value of 11111111.</p>
        <h3> Second-Chance Algorithm</h3>
        <p>The basic algorithm of second-chance replacement is a FIFO replacement algorithm. When a page gets a second chance, its reference bit is cleared and its arrival e is reset to the current time.</p>
        <h3> Enhanced Second-Chance Algorithm</h3>
        <p>The second-chance algorithm described above can be enhanced by considering troth the reference bit and the modify bit as an ordered pair.</p>
        <ol>
          <li>(0,0) neither recently used nor modified best page to replace.</li>
        </ol>
        <p>&nbsp;</p>
        <p>its directory structure that a file system is mounted at the specified mount point. This scheme enables the operating system to traverse its directory structure, switching among file systems as appropriate.</p>
        <h2>Allocation Methods</h2>
        <p> The direct-access nature of disks allows us flexibility in the implementation of files. Three major methods of allocating disk space are in wide use: contiguous, linked and indexed. Each method has its advantages and disadvantages.</p>
        <h2>Contiguous Allocation</h2>
        <ul>
          <li>The contiguous allocation method requires each file to occupy a set of contiguous blocks on the disk. Disk addresses define a linear ordering on the disk. Notice that with this ordering assuming that only one job is accessing the disk, accessing block b + 1 after block b normally requires no head movement.</li>
          <li>When head movement is needed, it is only one track. Thus, the number of disk seeks required for accessing contiguously allocated files is minimal.</li>
          <li>Contiguous allocation of a file is defined by the disk address and length (in block units) of the first block. If the file is n blocks long, and starts at location!), then it occupies blocks b, b + 1, b + 2, ..., b + n &ndash; 1. The directory entry for each file indicates the address of the starting block and the length of the area allocate for this file.</li>
          <li>Accessing a file that has been allocated contiguously is easy. For sequential access, the file system remembers the disk address of the last block referenced and, when necessary, reads the next block. For direct access to block i of a file that starts at block b, we can immediately access block b + i. The contiguous disk-space-allocation problem can be seen to be a particular application of the general dynamic storage-allocation First Fit and Best Fit are the most common strategies used to select a free hole from the set of available holes. Simulations have shown that both first-fit and best-fit are more efficient than worst-fit in terms of both time and storage utilization. Neither first-fit nor best-fit is clearly best in terms of storage utilization, but first-fit is generally faster.</li>
          <li>These algorithms suffer from the problem of external fragmentation. As files are allocated and deleted, the free disk space is broken into little pieces. External fragmentation exists whenever free space is broken into chunks. It becomes a problem when the largest contiguous chunks is insufficient for a request; storage is fragmented into a number of holes, no one of which is large enough to store the data. Depending on the total amount of disk storage and the average file size, external fragmentation may be either a minor or a major problem.</li>
          <li>To prevent loss of significant amounts of disk space to external fragmentation, the user had to run repacking routine that copied the entire file system onto another floppy disk or onto a tape. The original floppy disk was then freed completely, creating one large contiguous free space. The routine then copied the files back onto the floppy disk by allocating contiguous space from this one large hole. This scheme effectively compacts all free space into one contiguous space, solving the fragmentation problem. The cost of this compaction is time.</li>
          <li>The time cost is particularly severe for large hard disks that use contiguous allocation, where compacting all the space may take hours and may be necessary on a weekly basis. During this down time, normal system operation generally cannot be permitted, so such compaction is avoided at all costs on production machines.</li>
          <li>A major problem is determining how much space is needed for a file. When the file is created, the total amount of space it will need must be found and allocated.</li>
          <li>The user will normally over estimate the amount of space needed, resulting in considerable wasted space.</li>
        </ul>
        <h2>Linked Allocation</h2>
        <ul>
          <li>Linked allocation solves all problems of contiguous allocation. With link allocation, each file is a linked list disk blocks; the disk blocks may be scattered anywhere on the disk.</li>
          <li>This pointer is initialized to nil (the end-of-list pointer value) to signify an empty file. The size field is also set to 0. A write to the file causes a free bio to be found via the free-space management system, and this new block is the written to, and is linked to the end of the file</li>
          <li>There is no external fragmentation with linked allocation, and any free! block on the free-space list can be used to satisfy a request. Notice also that there is no need to declare the size of a file when that file is created. A file can continue to grow as long as there are free blocks. Consequently, it is never necessary to compact disk space.</li>
          <li>The major problem is that it can be used effectively for only sequential access files. To find the ith block of a file we must start at the beginning of that file, and follow the pointers until we get to the ith block. Each access to a pointer requires a disk read and sometimes a disk seek. Consequently, it is inefficient to support a direct-access capability for linked allocation files.</li>
          <li>Linked allocation is the space required for the pointers If a pointer requires 4 bytes out of a 512 Byte block then 0.78 percent of the disk is being used for pointer, rather than for information.</li>
          <li>The usual solution to this problem is to collect blocks into multiples, called clusters, and to allocate the clusters rather than blocks. For instance, the file system define a cluster as 4 blocks and operate on the disk in only cluster units.</li>
          <li>Pointers then use a much smaller percentage of the file's disk space. This method allows the logical-to-physical block mapping to remain simple, but improves disk throughput (fewer disk head seeks) and decreases the space needed for block allocation and free-list management. The cost of this approach an increase in internal fragmentation.</li>
          <li>Yet another problem is reliability. Since the files are linked together by pointers scattered all over the disk, consider what would happen if a pointer&mdash; were lost or damaged. Partial solutions are to use doubly linked lists or to store the file name and relative block number in each block; however, these schemes require even more overhead for each file.</li>
          <li>An important variation, on the linked allocation method is the use of a file allocation table (FAT). This simple but efficient method of disk-space allocation is used by the MS-DOS and OS/2 operating systems. A section of disk at the beginning of each-partition is set aside to contain the table. The table has one entry for each disk block, and is indexed by block number. The FAT is used much as is a linked list. The directory entry contains the block number of the first block of the file. The table entry indexed by that block number then contains the block number of the next block in the file. This chain continues until the last block, which has a special endof-file value -as the table entry. Unused blocks are indicated by a 0 table value. Allocating a new block to a file is a simple matter of finding the first 0-valued table entry, and replacing the previous end-of-file value with the address of the new block. The 0 is then replaced with the end-offile value. An illustrative example is the FAT structure of for a file consisting of disk blocks 217, 618, and 339.</li>
        </ul>
        <h2>Indexed Allocation</h2>
        <ul>
          <li>Linked allocation solves the external-fragmentation and sizedeclaration problems of contiguous allocation. The absence of a FAT, linked allocation cannot support efficient direct access, since the pointers to the blocks are scattered with the blocks themselves all over the disk and need to be retrieved in order Indexed allocation solves this problem by bringing all the pointers together into one location: the index block.</li>
          <li>Each file has its own index block, which is an array of diskblock addresses. The i<sup>th </sup>entry in the index block points to the i<sup>th </sup>block of the file. The directory contains the address of the index block.</li>
          <li>When the file is created, all pointers in the index block are set to nil. When the i<sup>th </sup>block is first written, a block is obtained: from the free space manager, and its address- is put in the i<sup>th </sup>index-block entry.</li>
          <li>Allocation supports direct access, without suffering from external fragmentation because any free block on he disk may satisfy a request for more space.</li>
          <li>Indexed allocation does suffer from wasted space. The pointer overhead of the index block is generally greater than the pointer overhead of linked allocation.
            <ol>
              <li><strong>Linked scheme. </strong>An index block is normally one disk block. Thus, it can be read and written directly by itself.</li>
              <li><strong>Multilevel index. </strong>A variant of the linked representation is to use a first-level index block to point to a set of second-level index blocks, which in turn point to the file blocks. To access a block, the operating system uses the first-level index to find a second-level index block, and that block to find the desired data block.</li>
            </ol>
          </li>
        </ul>
        <h1>FREE-SPACE MANAGEMENT</h1>
        <p>Since there is only a limited amount of disk space, it is necessary to reuse the space from deleted files for new files, if possible.</p>
        <h2>Bit Vector</h2>
        <ul>
          <li>Free-space list is implemented as a bit map or bit vector. Each block is represented by 1 bit. If the block is free, the bit is 1; if the block is allocated, the bit is 0.</li>
        </ul>
        <p>&nbsp;</p>
        <ul>
          <li><strong>Rotational Delay </strong>Disks, other than floppy disks, rotate at speeds ranging from 3600 rpm up to, as of this writing, 15,000 rpm; at this latter speed, there is one revolution per 4 ms. Thus, on the average, the rotational delay will be 2 ms. Floppy disks typically rotate at between 300 and 600 rpm. Thus the average delay will be between 100 and 50 ms.</li>
          <li><strong>Transfer Time </strong>The transfer time to or from the disk depends on the rotation speed of the disk in the following fashion: <strong>T= b/rN </strong>where T = transfer time</li>
        </ul>
        <p>b = number of bytes to be transferred N = number of bytes on a track r = rotation speed, in revolutions per second</p>
        <p>Thus the total average access time can be expressed as Ta = Ts +</p>
        <p>where Ts is the average seek time.</p>
        <h1>DISK SCHEDULING</h1>
        <ul>
          <li>The amount of head needed to satisfy a series of I/O request can affect the performance. If desired disk drive and controller are available, the request can be serviced immediately. If a device or controller is busy, any new requests for service will be placed on the queue of pending requests for that drive. When one request is completed, the operating system chooses which pending request to service next.</li>
          <li>Different types of scheduling algorithms are as follows.
            <ol>
              <li>First Come, First Served scheduling algorithm(FCFS).</li>
              <li>Shortest Seek Time First (SSTF) algorithm</li>
              <li>SCAN algorithm</li>
              <li>Circular SCAN (C-SCAN) algorithm</li>
              <li>Look Scheduling Algorithm</li>
            </ol>
          </li>
        </ul>
        <p><strong>First Come, First Served scheduling algorithm(FCFS).</strong></p>
        <ul>
          <li>The simplest form of scheduling is first-in-first-out (FIFO) scheduling, which processes items from the queue in sequential order. This strategy has the advantage of being fair, because every request is honored and the requests are honored in the order received. With FIFO, if there are only a few processes that require access and if many of the requests are to clustered file sectors, then we can hope for good performance.</li>
          <li><strong>Priority </strong>With a system based on priority (PRI), the control of the scheduling is outside the control of disk management software.</li>
          <li><strong>Last In First Out </strong>ln transaction processing systems, giving the device to the most recent user should result. In little or no arm movement for moving through a sequential file. Taking advantage of this locality improves throughput and reduces queue length.</li>
        </ul>
        <h2>Shortest Seek Time First (SSTF) algorithm</h2>
        <ul>
          <li>The SSTF policy is to select the disk I/O request the requires the least movement of the disk arm from its current position. <strong>Scan </strong>With the exception of FIFO, all of the policies described so far can leave some request unfulfilled until the entire queue is emptied. That is, there may always be new requests arriving that will be chosen before an existing request.</li>
          <li>The choice should provide better performance than FCFS algorithm.</li>
          <li>Under heavy load, SSTF can prevent distant request from ever being serviced. This phenomenon is known as starvation. SSTF scheduling is essentially a from of shortest job first scheduling. SSTF scheduling algorithm are not very popular because of two reasons.
            <ol>
              <li>Starvation possibly exists.</li>
              <li>it increases higher overheads.</li>
            </ol>
          </li>
        </ul>
        <h2> SCAN scheduling algorithm</h2>
        <ul>
          <li>The scan algorithm has the head start at track 0 and move towards the highest numbered track, servicing all requests for a track as it passes the track. The service direction is then reserved and the scan proceeds in the opposite direction, again picking up all requests in order.</li>
          <li>SCAN algorithm is guaranteed to service every request in one complete pass through the disk. SCAN algorithm behaves almost identically with the SSTF algorithm. The SCAN algorithm is sometimes called elevator algorithm.</li>
        </ul>
        <h2> C SCAN Scheduling Algorithm</h2>
        <ul>
          <li>The C-SCAN policy restricts scanning to one direction only. Thus, when the last track has been visited in one direction,</li>
        </ul>
        <p>the arm is returned to the opposite end of the disk and the scan begins again.</p>
        <ul>
          <li>This reduces&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; the&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; maximum&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; delay&nbsp;&nbsp; experienced&nbsp;&nbsp;&nbsp; by&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; new requests.</li>
        </ul>
        <h2>LOOK Scheduling Algorithm</h2>
        <p> Start the head moving in one direction. Satisfy the request for the closest track in that direction when there is no more request in the direction, the head is traveling, reverse direction and repeat. This algorithm is similar to innermost and outermost track on each circuit.</p>
        <h1> DISK MANAGEMENT</h1>
        <p>Operating system is responsible for disk management. Following are some activities discussed.</p>
        <p><strong> Disk Formatting </strong>Disk formatting is of two types.</p>
        <ol>
          <li>Physical formatting or low level formatting.</li>
          <li>Logical Formatting</li>
        </ol>
        <h2>Physical Formatting</h2>
        <ul>
          <li>Disk must be formatted before storing data.</li>
          <li>Disk must be divided into sectors that the disk controllers can read/write.</li>
          <li>Low level formatting files the disk with a special data structure for each sector.</li>
          <li>Data structure consists of three fields: header, data area and trailer.</li>
          <li>Header and trailer&nbsp;&nbsp; contain information&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; used&nbsp;&nbsp;&nbsp; by&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; the&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disk controller.</li>
          <li>Sector number and Error Correcting Codes (ECC) contained in the header and trailer.</li>
          <li>For writing data to the sector &ndash; ECC is updated.</li>
          <li>For reading data from the sector &ndash; ECC is recalculated.</li>
          <li>Low level formatting is done at factory.</li>
        </ul>
      </div>

    </div>
    <footer class="footer">
        <div class="container">
            <div class="row">
                <div class="footer-col">
                    <div class="webLogo">
                        <h2 class="logo">MCA_<span class="notes_text"> Notes</span></h2>
                    </div>
                </div>
                <div class="footer-col">
                    <h4>get help</h4>
                    <ul>
                        <li><a href="#">About US</a></li>
                        <li><a href="#">FAQ</a></li>
                        <li><a href="#">Privacy Policy</a></li>
                        <li><a href="#">Terms & Condition</a></li>

                    </ul>
                </div>
                <div class="footer-col">
                    <h4>All Link</h4>
                    <ul>
                        <li><a href="#">Home</a></li>
                        <li><a href="#">Contact US</a></li>
                        <li><a href="#">About Us</a></li>
                        <li><a href="#">Syllabus</a></li>
                        <li><a href="#">Quetion Paper</a></li>
                    </ul>
                </div>
                <div class="footer-col">
                    <h4>follow us</h4>
                    <div class="social-links">
                        <a href="#"><i class="fab fa-facebook-f"></i></a>
                        <a href="#"><i class="fab fa-twitter"></i></a>
                        <a href="#"><i class="fab fa-instagram"></i></a>
                        <a href="#"><i class="fab fa-linkedin-in"></i></a>
                    </div>
                </div>
                
            </div>
        </div>
        
    </footer>
    <script src="/gSap.js"></script>
    <script src="/script.js"></script>
</body>

</html>